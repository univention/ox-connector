#!/usr/bin/python3
# -*- coding: utf-8 -*-

# Copyright 2020 Univention GmbH
#
# http://www.univention.de/
#
# All rights reserved.
#
# The source code of this program is made available
# under the terms of the GNU Affero General Public License version 3
# (GNU AGPL V3) as published by the Free Software Foundation.
#
# Binary versions of this program provided by Univention to you as
# well as other copyrighted, protected or trademarked materials like
# Logos, graphics, fonts, specific documentations and configurations,
# cryptographic keys etc. are subject to a license agreement between
# you and Univention and not subject to the GNU AGPL V3.
#
# In the case you use this program under the terms of the GNU AGPL V3,
# the program is provided in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public
# License with the Debian GNU/Linux or Univention distribution in file
# /usr/share/common-licenses/AGPL-3; if not, see
# <http://www.gnu.org/licenses/>.

"""
Entry point for listener_trigger script.
"""

import dbm.gnu
import json
import logging
import sys
import time
import traceback
import os
from contextlib import contextmanager
from pathlib import Path

import univention.ox.provisioning.helpers as helpers
from univention.ox.provisioning import run

APP = "ox-connector"
DATA_DIR = Path("/var/lib/univention-appcenter/apps", APP, "data")
NEW_FILES_DIR = DATA_DIR / "listener"
OLD_FILES_DIR = NEW_FILES_DIR / "old"

logger = logging.getLogger("univention.ox")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(name)s: %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)
logger = logging.getLogger("listener")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)


def _get_old_object(dn):
    path_to_old_user = mapping.get(dn)
    if not path_to_old_user:
        return None
    path_to_old_user = Path(path_to_old_user.decode("utf-8"))
    logger.info(f"Loading old object from {path_to_old_user}")
    return object_from_path(path_to_old_user)


helpers.get_old_obj = _get_old_object


def load_from_json_file(path):
    """
    Just a helper function to get JSON content from a file, if it
    exists
    """
    if not path.exists():
        return None
    with path.open() as fd:
        return json.load(fd)


class TriggerObject(object):
    """
    A thin wrapper over a JSON file. Holds all the information from that
    file. May also hold information of this object from a previous run
    (needs a second, backup file for that)
    """
    def __init__(
        self, entry_uuid, object_type, dn, attributes, options, path=None
    ):
        self.entry_uuid = entry_uuid
        self.object_type = object_type
        self.dn = dn
        self.attributes = attributes
        self.options = options
        self.old_dn = None
        self.old_attributes = None
        self.old_options = None
        self.path = path  # file where it originates from
        self._old_loaded = False
        self._enriched = {}

    def ordering(self, other):
        """Sort incoming objects if needed. Return True whenever self should be
        placed BEFORE other. So return False would be the default."""
        sorted_object_types = [
            "oxmail/oxcontext",
            "users/user",
            "groups/group",
            "oxmail/functional_account",
            "oxresources/oxresources",
        ]
        if self.object_type == "oxmail/oxcontext" and self.attributes is None:
            # deleting a context should always come last
            return False
        if (
            (other.object_type == "oxmail/oxcontext") and
            (other.attributes is None)
        ):
            # deleting a context should always come last
            return True
        if (
            (self.object_type == "oxmail/accessprofile") or
            (other.object_type == "oxmail/accessprofile")
        ):
            # do not change order in accessprofiles
            return False
        lidx = sorted_object_types.index(self.object_type)
        ridx = sorted_object_types.index(other.object_type)
        return lidx < ridx

    def get_old_file_path(self):
        """
        Name of the old file. May or may not be present. Is used by
        objects_from_files to move files if specified.
        """
        return OLD_FILES_DIR / "{}.json".format(self.entry_uuid)

    def load_old(self):
        """Loads the old filename and sets attributes accordingly"""
        old_path = self.get_old_file_path()
        logger.info(f"Looking for old data in {old_path}")
        content = load_from_json_file(old_path)
        if content:
            self.old_dn = content["dn"]
            self.old_attributes = content["object"]
            self.old_options = content["options"]
        self._old_loaded = True

    def was_added(self):
        """
        Whether this object is new. Needs the have read an old file
        for this to give a meaningful response
        """
        if not self._old_loaded:
            return None
        return self.old_dn is None

    def was_modified(self):
        """
        Whether this object was modified. Needs the have read an old
        file for this to give a meaningful response
        """
        if self.was_deleted():
            return False
        if self._old_loaded is False:
            return None
        return not self.was_added() and not self.was_deleted()

    def was_deleted(self):
        """Whether this object was deleted."""
        return self.attributes is None

    def was_enriched(self):
        """Whether the obj was modified via set_attr during processing."""
        return bool(self._enriched)

    def set_attr(self, attr, value):
        """Enrich an object's attributes. Will be preserved should the
        old object be kept (i.e. files are moved at the end)"""
        if self.attributes.get(attr) != value:
            self.attributes[attr] = value
            self._enriched[attr] = value

    def dump(self, filename=None):
        """Save the object into a filename."""
        content = {
            "id": self.entry_uuid,
            "udm_object_type": self.object_type,
            "dn": self.dn,
            "object": self.attributes,
            "options": self.options,
        }
        filename = filename or self.path
        with open(f"{filename}.tmp", "w") as fd:
            os.fchmod(fd.fileno(), 0o600)
            json.dump(content, fd, sort_keys=True, indent=4)
        os.replace(f"{filename}.tmp", filename)

    def __repr__(self):
        return "Object({!r}, {!r})".format(self.object_type, self.dn)


class KeyValueStore(object):
    """
    Database about meta information on this listener.
    Particularly the number of consecutive errors.
    """
    def __init__(self, name):
        self.db_fname = str(NEW_FILES_DIR / name)

    @contextmanager
    def open(self):
        with dbm.gnu.open(self.db_fname, "cs") as db:
            yield db

    def set(self, dn, path):
        if dn is None:
            return
        with self.open() as db:
            if path is None:
                if dn in db:
                    del db[dn]
            else:
                db[dn] = str(path)

    def get(self, key):
        with self.open() as db:
            return db.get(key)


meta = KeyValueStore(
    "meta.db"
)  # arbitrary data, at the moment only stores consecutive errors
mapping = KeyValueStore("old.db")  # stores dn -> path to last json file


def object_from_path(path):
    """Extract object information from JSON file"""
    content = load_from_json_file(path)
    entry_uuid = content["id"]
    object_type = content["udm_object_type"]
    dn = content["dn"]
    attributes = content["object"]
    options = content["options"]
    obj = TriggerObject(entry_uuid, object_type, dn, attributes, options, path)
    return obj


def objects_from_files(delete_files=True, move_files=False):
    """
    Iterates over all JSON files and yields a TriggerObject. After the
    caller is done with it, it can delete or move the file. If it moves the
    file, a copy of this very JSON file is created so that a new run can
    reload it (useful if you need to act on various changes in
    attributes)
    """
    objs = []
    for path in sorted(NEW_FILES_DIR.glob("*.json")):
        obj = object_from_path(path)
        if move_files:
            obj.load_old()
        if obj.attributes is None and obj.old_attributes is None:
            # happens when creation and deletion happens within one
            # "listener cycle" => nothing happened
            path.unlink()
        else:
            idx = 0
            for _obj, _ in objs:
                if obj.ordering(_obj):
                    break
                idx += 1
            objs.insert(idx, (obj, path))

    for obj, path in objs:
        yield obj
        if move_files:
            mapping.set(obj.old_dn, None)
            old_file_path = obj.get_old_file_path()
            if old_file_path.exists():
                old_file_path.unlink()
            if obj.was_deleted():
                logger.info(f"Object was deleted. Deleting {path}")
                path.unlink()
                mapping.set(obj.dn, None)
            else:
                logger.info(f"mv {path} -> {old_file_path}")
                old_file_path.parent.mkdir(parents=True, exist_ok=True)
                path.replace(old_file_path)
                if obj.was_enriched():
                    logger.info(
                        "... not done yet; enriching object by rewriting JSON file..."
                    )
                    obj.dump(old_file_path)
                mapping.set(obj.dn, old_file_path)
        elif delete_files:
            logger.info(f"Deleting {path}")
            path.unlink()


def run_on_files(
    objs,
    f,
    stop_at_first_error=True,
    pause_after_errors_num=0,
    pause_after_errors_length=0,
):
    """
    Iterate over objects (returned by objects_from_files) and runs a
    function f on this object. May continue to do so even if one iteration
    failed. Returns the number of objects that were successfully processed OR
    an error code (in this case, the number is negative)
    """
    ret = 0
    seen = 0
    for obj in objs:
        logger.info(f"Handling {obj.path!r}")
        try:
            f(obj)
        except Exception:
            logger.warning(f"Error while processing {obj.path}")
            traceback.print_exc()
            ret = -1
            errors = meta.get("errors")
            if errors is None:
                errors = 1
            else:
                errors = int(errors) + 1
            meta.set("errors", str(errors))
            if stop_at_first_error:
                if 0 < pause_after_errors_num <= errors:
                    logger.warning(f"This is consecutive error #{errors}")
                    logger.warning(
                        f"Sleeping for {pause_after_errors_length} sec"
                    )
                    time.sleep(pause_after_errors_length)
                break
        else:
            meta.set("errors", "0")
            seen += 1
    logger.info(f"Successfully processed {seen} files during this run")
    if ret == -1:
        return ret
    return seen


def main():
    while True:
        objs = objects_from_files(delete_files=False, move_files=True)
        files_seen = run_on_files(
            objs, run, pause_after_errors_num=3, pause_after_errors_length=0
        )
        if files_seen <= 0:
            return -files_seen
    # never reached
    return 0


if __name__ == '__main__':
    sys.exit(main())

# [EOF]
